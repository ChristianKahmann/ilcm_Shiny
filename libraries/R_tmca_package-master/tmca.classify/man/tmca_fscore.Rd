% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluation.R
\name{tmca_fscore}
\alias{tmca_fscore}
\title{Classification evaluation scores
F1, Cohen's kappa, Krippendorff's alpha to compare two label vectors, prediction and truth.
Micro average: TP, FP, FN over all category decisions first, then F1
Macro average: F1 over each individual categories first, then average}
\usage{
tmca_fscore(prediction, truth, positive_class = NULL, evaluate_irr = TRUE)
}
\arguments{
\item{prediction}{vector (factor) of predicted labels}

\item{truth}{vector (factor) of true labels}

\item{positive_class}{label (level) of positive class (if not given, the minority class
in true labels is assumed as positive)}

\item{evaluate_irr}{compute alpha and kappa agreement statistics (requires irr package)}
}
\value{
Evaluation metrics: Precision, Recall, Specificity, Accuracy, F1-score, Alpha, Kappa
}
\description{
Classification evaluation scores
F1, Cohen's kappa, Krippendorff's alpha to compare two label vectors, prediction and truth.
Micro average: TP, FP, FN over all category decisions first, then F1
Macro average: F1 over each individual categories first, then average
}
\examples{
truth <- factor(c("P", "N", "N", "N"))
prediction <- factor(c("P", "P", "P", "N"))
tmca_fscore(prediction, truth, positive_class = "P")
}
